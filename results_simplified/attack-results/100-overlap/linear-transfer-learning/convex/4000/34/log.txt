Namespace(chk_path='chk-black', chk_subdir='poisons', device='cuda', dset_path='datasets', end2end=False, eval_poison_path='', gpu='2', lr_decay_epoch=[30, 45], mode='convex', model_resume_path='model-chks', nearest=False, net_repeat=1, num_per_class=50, original_grad=True, poison_decay_ites=[], poison_decay_ratio=0.1, poison_epsilon=0.1, poison_ites=4000, poison_label=8, poison_lr=0.04, poison_momentum=0.9, poison_num=5, poison_opt='adam', resume_poison_ite=0, retrain_bsize=64, retrain_epochs=60, retrain_lr=0.1, retrain_momentum=0.9, retrain_opt='adam', retrain_wd=0, subs_chk_name=['ckpt-%s-4800-dp0.200-droplayer0.000-seed1226.t7', 'ckpt-%s-4800-dp0.250-droplayer0.000-seed1226.t7', 'ckpt-%s-4800-dp0.300-droplayer0.000.t7'], subs_dp=[0.2, 0.25, 0.3], subset_group=0, substitute_nets=['DPN92', 'SENet18', 'ResNet50', 'ResNeXt29_2x64d', 'GoogLeNet', 'MobileNetV2'], target_index=34, target_label=6, target_net=['DPN92', 'SENet18', 'ResNet50', 'ResNeXt29_2x64d', 'GoogLeNet', 'MobileNetV2', 'ResNet18', 'DenseNet121'], test_chk_name='ckpt-%s-4800.t7', tol=1e-06, train_data_path='datasets/CIFAR10_TRAIN_Split.pth')
Path: chk-black/convex/4000/34
Selected base image indices: [213, 225, 227, 247, 249]
 2020-01-28 07:10:42 Iteration 0 	 Training Loss: 5.396e-01 	 Loss in Target Net: 4.846e-01	  
 2020-01-28 07:18:14 Iteration 50 	 Training Loss: 6.958e-03 	 Loss in Target Net: 1.294e-02	  
 2020-01-28 07:27:23 Iteration 100 	 Training Loss: 4.944e-03 	 Loss in Target Net: 2.934e-02	  
 2020-01-28 07:36:21 Iteration 150 	 Training Loss: 4.418e-03 	 Loss in Target Net: 2.786e-02	  
 2020-01-28 07:45:08 Iteration 200 	 Training Loss: 3.719e-03 	 Loss in Target Net: 2.916e-02	  
 2020-01-28 07:55:24 Iteration 250 	 Training Loss: 3.993e-03 	 Loss in Target Net: 2.136e-02	  
 2020-01-28 08:04:23 Iteration 300 	 Training Loss: 3.481e-03 	 Loss in Target Net: 1.779e-02	  
 2020-01-28 08:14:46 Iteration 350 	 Training Loss: 3.158e-03 	 Loss in Target Net: 1.551e-02	  
 2020-01-28 08:23:40 Iteration 400 	 Training Loss: 3.561e-03 	 Loss in Target Net: 1.294e-02	  
 2020-01-28 08:34:35 Iteration 450 	 Training Loss: 3.335e-03 	 Loss in Target Net: 1.670e-02	  
 2020-01-28 08:47:25 Iteration 500 	 Training Loss: 2.647e-03 	 Loss in Target Net: 1.572e-02	  
 2020-01-28 09:00:26 Iteration 550 	 Training Loss: 2.755e-03 	 Loss in Target Net: 2.324e-02	  
 2020-01-28 09:12:19 Iteration 600 	 Training Loss: 2.812e-03 	 Loss in Target Net: 1.930e-02	  
 2020-01-28 09:24:45 Iteration 650 	 Training Loss: 3.186e-03 	 Loss in Target Net: 1.732e-02	  
 2020-01-28 09:37:39 Iteration 700 	 Training Loss: 2.991e-03 	 Loss in Target Net: 1.182e-02	  
 2020-01-28 09:50:36 Iteration 750 	 Training Loss: 2.837e-03 	 Loss in Target Net: 1.682e-02	  
 2020-01-28 10:02:20 Iteration 800 	 Training Loss: 2.772e-03 	 Loss in Target Net: 1.355e-02	  
 2020-01-28 10:14:13 Iteration 850 	 Training Loss: 2.717e-03 	 Loss in Target Net: 1.451e-02	  
 2020-01-28 10:24:09 Iteration 900 	 Training Loss: 2.765e-03 	 Loss in Target Net: 2.181e-02	  
 2020-01-28 10:36:41 Iteration 950 	 Training Loss: 2.833e-03 	 Loss in Target Net: 1.692e-02	  
 2020-01-28 10:51:38 Iteration 1000 	 Training Loss: 2.467e-03 	 Loss in Target Net: 2.030e-02	  
 2020-01-28 11:04:41 Iteration 1050 	 Training Loss: 2.571e-03 	 Loss in Target Net: 2.008e-02	  
 2020-01-28 11:18:41 Iteration 1100 	 Training Loss: 2.790e-03 	 Loss in Target Net: 2.290e-02	  
 2020-01-28 11:31:02 Iteration 1150 	 Training Loss: 2.621e-03 	 Loss in Target Net: 2.135e-02	  
 2020-01-28 11:43:38 Iteration 1200 	 Training Loss: 2.843e-03 	 Loss in Target Net: 1.821e-02	  
 2020-01-28 11:57:29 Iteration 1250 	 Training Loss: 2.684e-03 	 Loss in Target Net: 1.957e-02	  
 2020-01-28 12:10:59 Iteration 1300 	 Training Loss: 2.543e-03 	 Loss in Target Net: 2.183e-02	  
 2020-01-28 12:26:36 Iteration 1350 	 Training Loss: 2.444e-03 	 Loss in Target Net: 2.352e-02	  
 2020-01-28 12:38:45 Iteration 1400 	 Training Loss: 2.783e-03 	 Loss in Target Net: 2.030e-02	  
 2020-01-28 12:52:10 Iteration 1450 	 Training Loss: 2.622e-03 	 Loss in Target Net: 1.886e-02	  
 2020-01-28 13:06:45 Iteration 1500 	 Training Loss: 3.144e-03 	 Loss in Target Net: 1.773e-02	  
 2020-01-28 13:20:38 Iteration 1550 	 Training Loss: 2.591e-03 	 Loss in Target Net: 1.584e-02	  
 2020-01-28 13:33:42 Iteration 1600 	 Training Loss: 2.365e-03 	 Loss in Target Net: 1.717e-02	  
 2020-01-28 13:47:50 Iteration 1650 	 Training Loss: 2.416e-03 	 Loss in Target Net: 1.594e-02	  
 2020-01-28 14:03:06 Iteration 1700 	 Training Loss: 2.283e-03 	 Loss in Target Net: 1.178e-02	  
 2020-01-28 14:15:06 Iteration 1750 	 Training Loss: 2.625e-03 	 Loss in Target Net: 6.348e-03	  
 2020-01-28 14:29:19 Iteration 1800 	 Training Loss: 2.426e-03 	 Loss in Target Net: 7.234e-03	  
 2020-01-28 14:41:59 Iteration 1850 	 Training Loss: 2.247e-03 	 Loss in Target Net: 8.247e-03	  
 2020-01-28 14:58:12 Iteration 1900 	 Training Loss: 2.776e-03 	 Loss in Target Net: 1.211e-02	  
 2020-01-28 15:12:28 Iteration 1950 	 Training Loss: 2.550e-03 	 Loss in Target Net: 1.401e-02	  
 2020-01-28 15:28:37 Iteration 2000 	 Training Loss: 2.252e-03 	 Loss in Target Net: 1.218e-02	  
 2020-01-28 15:45:12 Iteration 2050 	 Training Loss: 2.769e-03 	 Loss in Target Net: 1.123e-02	  
 2020-01-28 15:58:35 Iteration 2100 	 Training Loss: 2.301e-03 	 Loss in Target Net: 7.241e-03	  
 2020-01-28 16:13:40 Iteration 2150 	 Training Loss: 2.545e-03 	 Loss in Target Net: 7.842e-03	  
 2020-01-28 16:27:12 Iteration 2200 	 Training Loss: 2.757e-03 	 Loss in Target Net: 6.370e-03	  
 2020-01-28 16:42:39 Iteration 2250 	 Training Loss: 2.447e-03 	 Loss in Target Net: 5.376e-03	  
 2020-01-28 16:56:39 Iteration 2300 	 Training Loss: 2.850e-03 	 Loss in Target Net: 4.279e-03	  
 2020-01-28 17:12:36 Iteration 2350 	 Training Loss: 2.686e-03 	 Loss in Target Net: 6.312e-03	  
 2020-01-28 17:26:33 Iteration 2400 	 Training Loss: 2.508e-03 	 Loss in Target Net: 7.149e-03	  
 2020-01-28 17:40:20 Iteration 2450 	 Training Loss: 2.821e-03 	 Loss in Target Net: 6.223e-03	  
 2020-01-28 17:57:17 Iteration 2500 	 Training Loss: 2.078e-03 	 Loss in Target Net: 7.226e-03	  
 2020-01-28 18:13:13 Iteration 2550 	 Training Loss: 2.382e-03 	 Loss in Target Net: 5.669e-03	  
 2020-01-28 18:26:48 Iteration 2600 	 Training Loss: 2.426e-03 	 Loss in Target Net: 9.724e-03	  
 2020-01-28 18:43:51 Iteration 2650 	 Training Loss: 2.486e-03 	 Loss in Target Net: 6.317e-03	  
 2020-01-28 18:57:07 Iteration 2700 	 Training Loss: 2.403e-03 	 Loss in Target Net: 7.361e-03	  
 2020-01-28 19:10:28 Iteration 2750 	 Training Loss: 2.750e-03 	 Loss in Target Net: 6.923e-03	  
 2020-01-28 19:24:22 Iteration 2800 	 Training Loss: 2.668e-03 	 Loss in Target Net: 6.535e-03	  
 2020-01-28 19:40:11 Iteration 2850 	 Training Loss: 2.919e-03 	 Loss in Target Net: 4.721e-03	  
 2020-01-28 19:54:49 Iteration 2900 	 Training Loss: 2.508e-03 	 Loss in Target Net: 4.002e-03	  
 2020-01-28 20:07:49 Iteration 2950 	 Training Loss: 2.950e-03 	 Loss in Target Net: 3.638e-03	  
 2020-01-28 20:18:16 Iteration 3000 	 Training Loss: 2.610e-03 	 Loss in Target Net: 5.924e-03	  
 2020-01-28 20:29:35 Iteration 3050 	 Training Loss: 2.730e-03 	 Loss in Target Net: 3.962e-03	  
 2020-01-28 20:39:57 Iteration 3100 	 Training Loss: 2.639e-03 	 Loss in Target Net: 7.543e-03	  
 2020-01-28 20:49:39 Iteration 3150 	 Training Loss: 2.415e-03 	 Loss in Target Net: 8.031e-03	  
 2020-01-28 21:00:13 Iteration 3200 	 Training Loss: 2.577e-03 	 Loss in Target Net: 1.233e-02	  
 2020-01-28 21:11:00 Iteration 3250 	 Training Loss: 2.499e-03 	 Loss in Target Net: 4.701e-03	  
 2020-01-28 21:22:17 Iteration 3300 	 Training Loss: 2.872e-03 	 Loss in Target Net: 4.281e-03	  
 2020-01-28 21:33:31 Iteration 3350 	 Training Loss: 3.137e-03 	 Loss in Target Net: 8.434e-03	  
 2020-01-28 21:45:10 Iteration 3400 	 Training Loss: 2.398e-03 	 Loss in Target Net: 1.574e-02	  
 2020-01-28 21:55:02 Iteration 3450 	 Training Loss: 2.509e-03 	 Loss in Target Net: 8.006e-03	  
 2020-01-28 22:05:05 Iteration 3500 	 Training Loss: 2.856e-03 	 Loss in Target Net: 6.684e-03	  
 2020-01-28 22:15:03 Iteration 3550 	 Training Loss: 2.552e-03 	 Loss in Target Net: 7.309e-03	  
 2020-01-28 22:25:17 Iteration 3600 	 Training Loss: 2.808e-03 	 Loss in Target Net: 7.797e-03	  
 2020-01-28 22:36:46 Iteration 3650 	 Training Loss: 3.027e-03 	 Loss in Target Net: 5.953e-03	  
 2020-01-28 22:47:06 Iteration 3700 	 Training Loss: 2.503e-03 	 Loss in Target Net: 6.524e-03	  
 2020-01-28 22:57:21 Iteration 3750 	 Training Loss: 2.423e-03 	 Loss in Target Net: 5.002e-03	  
 2020-01-28 23:05:31 Iteration 3800 	 Training Loss: 2.437e-03 	 Loss in Target Net: 5.013e-03	  
 2020-01-28 23:12:14 Iteration 3850 	 Training Loss: 2.552e-03 	 Loss in Target Net: 5.773e-03	  
 2020-01-28 23:21:06 Iteration 3900 	 Training Loss: 2.445e-03 	 Loss in Target Net: 5.924e-03	  
 2020-01-28 23:30:22 Iteration 3950 	 Training Loss: 2.750e-03 	 Loss in Target Net: 7.071e-03	  
 2020-01-28 23:39:51 Iteration 3999 	 Training Loss: 2.572e-03 	 Loss in Target Net: 7.121e-03	  
Evaluating against victims networks
DPN92
Using Adam for retraining
Files already downloaded and verified
2020-01-28 23:39:58, Epoch 0, Iteration 7, loss 3.126 (4.227), acc 80.769 (65.400)
2020-01-28 23:39:58, Epoch 30, Iteration 7, loss 0.033 (0.195), acc 98.077 (98.200)
Target Label: 6, Poison label: 8, Prediction:8, Target's Score:[3.4788291, -17.136992, -69.928024, 4.7821355, -35.74847, 2.4756575, 22.306301, -54.379612, 26.755774, -131.6443], Poisons' Predictions:[8, 6, 8, 8, 8]
2020-01-28 23:40:03 Epoch 59, Val iteration 0, acc 90.200 (90.200)
2020-01-28 23:40:11 Epoch 59, Val iteration 19, acc 91.200 (91.580)
* Prec: 91.58000221252442
--------
SENet18
Using Adam for retraining
Files already downloaded and verified
2020-01-28 23:40:15, Epoch 0, Iteration 7, loss 0.549 (0.868), acc 88.462 (85.400)
2020-01-28 23:40:16, Epoch 30, Iteration 7, loss 0.134 (0.134), acc 96.154 (96.600)
Target Label: 6, Poison label: 8, Prediction:6, Target's Score:[0.3892259, -5.547278, -8.785858, -0.9589315, 4.0212145, -3.7714474, 20.74618, -2.736425, 12.263008, -5.7415833], Poisons' Predictions:[8, 8, 8, 8, 8]
2020-01-28 23:40:17 Epoch 59, Val iteration 0, acc 92.400 (92.400)
2020-01-28 23:40:20 Epoch 59, Val iteration 19, acc 92.400 (91.400)
* Prec: 91.4000015258789
--------
ResNet50
Using Adam for retraining
Files already downloaded and verified
2020-01-28 23:40:24, Epoch 0, Iteration 7, loss 0.074 (0.775), acc 94.231 (91.400)
2020-01-28 23:40:25, Epoch 30, Iteration 7, loss 0.000 (0.046), acc 100.000 (99.400)
Target Label: 6, Poison label: 8, Prediction:6, Target's Score:[-44.329517, -147.915, -125.164764, -31.442156, -64.11417, -38.896893, 12.547949, -56.84664, 9.383613, -66.25604], Poisons' Predictions:[8, 8, 8, 8, 8]
2020-01-28 23:40:27 Epoch 59, Val iteration 0, acc 93.400 (93.400)
2020-01-28 23:40:32 Epoch 59, Val iteration 19, acc 93.800 (93.050)
* Prec: 93.05000190734863
--------
ResNeXt29_2x64d
Using Adam for retraining
Files already downloaded and verified
2020-01-28 23:40:36, Epoch 0, Iteration 7, loss 0.801 (2.360), acc 82.692 (66.800)
2020-01-28 23:40:37, Epoch 30, Iteration 7, loss 0.005 (0.014), acc 100.000 (99.200)
Target Label: 6, Poison label: 8, Prediction:8, Target's Score:[-23.824574, -17.046446, -8.773781, 10.4844475, -51.35109, -22.286314, 11.248552, -14.805371, 15.386452, -38.554115], Poisons' Predictions:[8, 8, 8, 8, 8]
2020-01-28 23:40:38 Epoch 59, Val iteration 0, acc 92.400 (92.400)
2020-01-28 23:40:44 Epoch 59, Val iteration 19, acc 93.200 (92.440)
* Prec: 92.44000205993652
--------
GoogLeNet
Using Adam for retraining
Files already downloaded and verified
2020-01-28 23:40:48, Epoch 0, Iteration 7, loss 0.951 (0.473), acc 86.538 (89.600)
2020-01-28 23:40:49, Epoch 30, Iteration 7, loss 0.102 (0.063), acc 94.231 (97.400)
Target Label: 6, Poison label: 8, Prediction:8, Target's Score:[-22.12717, -11.250628, -8.606841, -3.4787593, -17.183466, -5.0625496, 13.051662, 1.7851931, 13.309395, -25.462437], Poisons' Predictions:[8, 8, 8, 8, 8]
2020-01-28 23:40:51 Epoch 59, Val iteration 0, acc 90.600 (90.600)
2020-01-28 23:40:58 Epoch 59, Val iteration 19, acc 91.400 (91.920)
* Prec: 91.92000160217285
--------
MobileNetV2
Using Adam for retraining
Files already downloaded and verified
2020-01-28 23:41:01, Epoch 0, Iteration 7, loss 0.834 (2.577), acc 86.538 (68.400)
2020-01-28 23:41:02, Epoch 30, Iteration 7, loss 0.104 (0.415), acc 96.154 (92.200)
Target Label: 6, Poison label: 8, Prediction:8, Target's Score:[-2.0648663, -4.968791, -4.4574122, -0.15478194, -52.682343, -8.966708, -1.4302303, -9.054025, 23.303602, -31.727577], Poisons' Predictions:[6, 8, 8, 8, 8]
2020-01-28 23:41:03 Epoch 59, Val iteration 0, acc 87.200 (87.200)
2020-01-28 23:41:05 Epoch 59, Val iteration 19, acc 88.000 (86.720)
* Prec: 86.72000198364258
--------
ResNet18
Using Adam for retraining
Files already downloaded and verified
2020-01-28 23:41:08, Epoch 0, Iteration 7, loss 1.046 (0.768), acc 92.308 (87.400)
2020-01-28 23:41:09, Epoch 30, Iteration 7, loss 0.028 (0.072), acc 100.000 (98.600)
Target Label: 6, Poison label: 8, Prediction:6, Target's Score:[-38.72629, -2.4011014, -19.043436, 1.6897382, -40.728985, -11.803339, 8.992368, -44.066776, 8.133584, -22.32811], Poisons' Predictions:[8, 8, 8, 8, 8]
2020-01-28 23:41:09 Epoch 59, Val iteration 0, acc 93.200 (93.200)
2020-01-28 23:41:12 Epoch 59, Val iteration 19, acc 93.800 (92.700)
* Prec: 92.7000015258789
--------
DenseNet121
Using Adam for retraining
Files already downloaded and verified
2020-01-28 23:41:15, Epoch 0, Iteration 7, loss 0.093 (0.474), acc 96.154 (91.200)
2020-01-28 23:41:16, Epoch 30, Iteration 7, loss 0.003 (0.003), acc 100.000 (100.000)
Target Label: 6, Poison label: 8, Prediction:8, Target's Score:[-5.6533284, -10.962052, -8.7897415, 0.23305143, -13.315902, -5.523111, 7.488314, -29.962925, 7.848133, -11.877916], Poisons' Predictions:[8, 8, 8, 8, 8]
2020-01-28 23:41:19 Epoch 59, Val iteration 0, acc 93.400 (93.400)
2020-01-28 23:41:24 Epoch 59, Val iteration 19, acc 93.600 (92.800)
* Prec: 92.80000228881836
--------
------SUMMARY------
TIME ELAPSED (mins): 989
TARGET INDEX: 34
DPN92 1
SENet18 0
ResNet50 0
ResNeXt29_2x64d 1
GoogLeNet 1
MobileNetV2 1
ResNet18 0
DenseNet121 1
